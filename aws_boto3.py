import boto3
import uuid

#initializing the s3_connection type
s3_client = boto3.client('s3')
s3_resource = boto3.resource('s3')

#method to add random generated suffix to bucket_name
def create_bucket_name(bucket_prefix):
    return ''.join([bucket_prefix, str(uuid.uuid4())])

#method to create bucket with bucket_name and s3_connection type
def create_bucket(bucket_prefix, s3_connection):
    session = boto3.session.Session()
    current_region = session.region_name
    bucket_name = create_bucket_name(bucket_prefix)
    bucket_response = s3_connection.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': current_region})
    
    print(bucket_name, current_region)
    #return bucket_name, bucket_response
    return bucket_name

#method to create a file with parameters as file_content and content size,
#also, prefixing file_name with first 6 characters of hexadecimal string generated by uuid
def create_temp_file(size, filename, file_content):
    random_file_name = ''.join([str(uuid.uuid4().hex[:6]), filename])
    with open(random_file_name, 'w') as f:
        f.write(str(file_content) * size)
    
    return random_file_name

first_bucket_name = create_bucket('first_bucket', s3_resource)
first_file_name = create_temp_file(10, 'irst_file.txt', "Hello, I am Hatish")

#creating instances of Bucket and Object
first_bucket = s3_resource.Bucket(name=first_bucket_name)
first_object = s3_resource.Object(bucket_name=first_bucket_name, key=first_file_name)

'''
#referencing Bucket and Object from each other instances
first_object_again = first_bucket.Object(first_file_name)
first_bucket_again = first_object.Bucket()
'''

#uploading the file in 4 ways
#using Object instance
s3_resource.Object(first_bucket_name, first_file_name).upload_file(FileName=first_file_name)
#using Object instance variable
first_object.upload_file(first_file_name)
#using Bucket instance
s3_resource.Bucket(first_bucket_name).upload_file(Filename=first_file_name, Key=first_file_name)
#using Client type
s3_resource.meta.client.upload_file(Filename=first_file_name, Bucket=first_bucket_name, Key=first_file_name)

#downloading the file
s3_resource.Object(first_bucket_name, first_file_name).download_file(f'/tmp/{first_file_name}')

#method to copy object between buckets
def copy_to_bucket(bucket_from_name, bucket_to_name, file_name):
    copy_source = {
        'Bucket': bucket_from_name,
        'Key': file_name
    }
    s3_resource.Object(bucket_to_name, file_name).copy(copy_source)

second_bucket_name = create_bucket('second_bucket', s3_resource)
copy_to_bucket(first_bucket_name, second_bucket_name, first_file_name)

#delete the object
s3_resource.Object(second_bucket_name, first_file_name).delete()

#uploading file with public-read access and access the object permissions
second_file_name = create_temp_file(10, 'second_file.txt', "Hello, I am Gudivada")
second_object = s3_resource.Object(first_bucket.name, second_file_name)
second_object.upload_file(second_file_name, ExtraArgs={'ACL': 'public-read'})
second_object_acl = second_object.Acl()
print(second_object_acl.grants)
#make it private again
second_object_acl.put(ACL='private')
print(second_object_acl.grants)

#uploading a file with Encryption enabled
third_file_name = create_temp_file(10, 'third_file.txt', "Namaste India...")
third_object = s3_resource.Object(first_bucket_name, third_file_name)
third_object.upload_file(third_file_name, ExtraArgs={'ServerSideEncryption': 'AES256'})
print(third_object.server_side_encryption)

#adding storage class configuration for the object
#need to recreate the exisiting object if you want to change for exisiting object
third_object.upload_file(third_file_name, ExtraArgs={'ServerSideEncryption': 'AES256', 'StorageClass': 'STANDARD_IA'})
#need to reload the local object to fetch the updated changes
third_object.reload()
print(third_object.storage_class)

#enabling versioning to buckets and uploading multiple versions of objects
def enable_bucket_versioning(bucket_name):
    bucket_versioning = s3_resource.BucketVersioning(bucket_name)
    bucket_versioning.enable()
    print(bucket_versioning.status)

enable_bucket_versioning(second_bucket_name)

#uploading 2 versions of first_file and 1 version of second_file
s3_resource.Object(second_bucket_name, first_file_name).upload_file(first_file_name)
s3_resource.Object(second_bucket_name, first_file_name).upload_file(third_file_name)
s3_resource.Object(second_bucket_name, second_file_name).upload_file(second_file_name)
#print the available version of objects
print(s3_resource.Object(second_bucket_name, first_file_name).version_id)

#traversing all the buckets with s3_resource
for bucket in s3_resource.buckets.all():
    print(bucket.name)

#traversing all the buckets with s3_client
for bucket_dict in s3_resource.meta.client.list_buckets().get('Buckets'):
    print(bucket_dict['Name'])

#traversing the objects in a bucket
for object in first_bucket.objects.all():
    sub_resource = object.Object()
    print(object.key, object.storage_class, object.last_modified, sub_resource.version_id, sub_resource.metadata)

#deleting all the objects (including the versions of it if exists) inside a bucket
def delete_all_objects(bucket_name):
    object_details = []
    bucket = s3_resource.Bucket(bucket_name)
    for object_version in bucket.object_versions.all():
        object_details.append({'Key': object_version.object_key, 'VersionId': object_version.id})

    print(object_details)
    bucket.delete_objects(Delete={'Objects': object_details})

delete_all_objects(first_bucket_name)
delete_all_objects(second_bucket_name)

#non-empty buckets cannot be deleted, will throw an exception
#deleting buckets with connection types
s3_resource.Bucket(first_bucket_name).delete()
s3_resource.meta.client.delete_bucket(Bucket=second_bucket_name)


''' 
Blog_Title: Python, Boto3, and AWS S3: Demystified
Blog_Link: https://realpython.com/python-boto3-aws-s3/
'''
